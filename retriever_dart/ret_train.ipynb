{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f47ec80-e266-41ae-ba64-84b3dcb3fd1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27179/3843401274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm.notebook as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ad9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수들\n",
    "from dart_prcs import *\n",
    "\n",
    "\n",
    "def stringfy(tmp):\n",
    "    tmp = eval(tmp) \n",
    "    return \" \".join( tmp )\n",
    "\n",
    "def preprocess(text):\n",
    "    import regex as re\n",
    "    text = remove_repeated_spacing(text)\n",
    "    text = clean_punc(text)\n",
    "    text = remove_useless_breacket(text)\n",
    "    text = remove_email(text)\n",
    "    text = remove_url(text)\n",
    "    text = re.sub(\"\\n\", \"\", text) \n",
    "    text = re.sub(\"\\\"\", \"\", text)\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    text = re.sub(\"-\", \"\", text)\n",
    "    text = re.sub(\"\\\"\", \"\", text)\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    text = re.sub(\",\", \"\", text)\n",
    "    text = re.sub(\"\\.\", \"\", text)\n",
    "    text = re.sub(\"(주)\", \"\", text)\n",
    "    text = re.sub(\"\\)\", \" \", text)\n",
    "    text = re.sub(\"\\(\", \" \", text)\n",
    "    \n",
    "    # text = re.sub(\"[(]\", \" (\", text)\n",
    "    # text = re.sub(\"[)]\", \") \", text)\n",
    "    text = re.sub('[■:%/~㈜↓&※·→①②$③○-ㆍ」「■>Ⅱ④;▶●?⑤社⑥⑦□=ㅇ『』外<◆△【】現▲▷美∼用☞@前㎡◇中Ⅲ－無新內％◈}株ㅁ會㎥{ㄱⅠ化高＋ㄴ日有：ㄷ公司全後限，〔〕學↑式月|＆ℓㄹ…業人名《》年^韓部▼本大小海│愛故食形㎏獨山多水東可非思州國家生上℃ㅂ合金發在同⊙軍英物實田開○○○○○作性體度産空分子光重ㅅ島間時利＂面母≪㎖資心別氣仁未京來對成雲淸聖命保的集史靑場法神正第一硏㎞ㅡ★太民如理出入下解得安平＝帝所市石門相方元政先富北木自車南地―｜求ㅎㅎ≫西長銀者規制女江福和ㅌ通主義村當代力㎝善原選色；古河──都能動歌〈〉不定ㅠ吉事理張數朝金㎜記書]'\\\n",
    "                      , \" \", text)\n",
    "    text = remove_repeated_spacing(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def prcs_news_dataset(dataset):\n",
    "    news_df = pd.DataFrame( {'title' : dataset['title'], 'original' : dataset['original'], 'category':dataset['category'],'article' : dataset['article'] } )\n",
    "    eco_df = news_df[ news_df['category'] == \"경제\" ]\n",
    "    eco_df['new_article'] = eco_df['original'].map(stringfy)\n",
    "    eco_df['new_article'] = eco_df['new_article'].map(preprocess)\n",
    "    return eco_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a076ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration aihub-news30k-4c3dbb7a134def57\n",
      "Reusing dataset csv (/opt/ml/.cache/huggingface/datasets/csv/aihub-news30k-4c3dbb7a134def57/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa39395dd62045b3a29e04a6e8f3c211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26117/1118650082.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eco_df['new_article'] = eco_df['original'].map(stringfy)\n",
      "/tmp/ipykernel_26117/1118650082.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eco_df['new_article'] = eco_df['new_article'].map(preprocess)\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 데이터 받아오기\n",
    "news_dataset = datasets.load_dataset('nlpHakdang/aihub-news30k',  data_files={\"train\":\"news_train_1_1.csv\", \"valid\":\"news_valid_1_1.csv\"}, use_auth_token='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')\n",
    "\n",
    "news_train_set = news_dataset['train']\n",
    "news_val_set = news_dataset['valid']\n",
    "\n",
    "\n",
    "\n",
    "new_tr_df = prcs_news_dataset( news_train_set )\n",
    "new_vl_df = prcs_news_dataset( news_val_set )\n",
    "\n",
    "# see\n",
    "# r = np.random.randint(len(new_tr_df))\n",
    "# new_tr_df.iloc[r]['new_article'],new_tr_df.iloc[r]['original']\n",
    "\n",
    "del new_tr_df['original']\n",
    "del new_tr_df['article']\n",
    "del new_vl_df['original']\n",
    "del new_vl_df['article']\n",
    "\n",
    "# new_tr_df.to_csv(\"news_train_ret_v0.csv\")\n",
    "# new_vl_df.to_csv(\"news_valid_ret_v0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf1b6193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration beneficiary-15e09db052e9f529\n",
      "Reusing dataset csv (/opt/ml/.cache/huggingface/datasets/csv/beneficiary-15e09db052e9f529/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c537356b58e24fe49d9f287a2a6688fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>주식 코드</th>\n",
       "      <th>report_idx</th>\n",
       "      <th>기업 코드</th>\n",
       "      <th>기업 이름</th>\n",
       "      <th>수정 일자</th>\n",
       "      <th>사업의 개요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>950130</td>\n",
       "      <td>20211115000151</td>\n",
       "      <td>956028</td>\n",
       "      <td>엑세스바이오</td>\n",
       "      <td>20170630</td>\n",
       "      <td>['당사는 체외진단 기술을 토대로 면역화학진단 바이오센서 분자진단 기술을 기반으로 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33030</td>\n",
       "      <td>20211115002044</td>\n",
       "      <td>232317</td>\n",
       "      <td>지오엠씨</td>\n",
       "      <td>20170630</td>\n",
       "      <td>['가  영업의 현황', '당사는 30년간 지속적으로 진행해 오고 있는 엠씨스퀘어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139200</td>\n",
       "      <td>20211115002442</td>\n",
       "      <td>855145</td>\n",
       "      <td>하이골드오션2호선박투자회사</td>\n",
       "      <td>20171127</td>\n",
       "      <td>['당사는 선박투자회사법에 따라 케이에스에프선박금융 가 발기인이 되어 설립되었으며 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>900070</td>\n",
       "      <td>20211125000049</td>\n",
       "      <td>783246</td>\n",
       "      <td>글로벌에스엠</td>\n",
       "      <td>20170703</td>\n",
       "      <td>['지회사 (持    Holding Company) 란 다른 회사의 식을 소유한 회...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>900120</td>\n",
       "      <td>20211028000448</td>\n",
       "      <td>800084</td>\n",
       "      <td>씨케이에이치</td>\n",
       "      <td>20170703</td>\n",
       "      <td>['가  산업개황 및 전망 당사의 매출과 이익은 중국 내에서의 건강식품 생산 및 판...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   주식 코드      report_idx   기업 코드           기업 이름     수정 일자  \\\n",
       "0           0  950130  20211115000151  956028          엑세스바이오  20170630   \n",
       "1           1   33030  20211115002044  232317            지오엠씨  20170630   \n",
       "2           3  139200  20211115002442  855145  하이골드오션2호선박투자회사  20171127   \n",
       "3           4  900070  20211125000049  783246          글로벌에스엠  20170703   \n",
       "4           5  900120  20211028000448  800084          씨케이에이치  20170703   \n",
       "\n",
       "                                              사업의 개요  \n",
       "0  ['당사는 체외진단 기술을 토대로 면역화학진단 바이오센서 분자진단 기술을 기반으로 ...  \n",
       "1  ['가  영업의 현황', '당사는 30년간 지속적으로 진행해 오고 있는 엠씨스퀘어 ...  \n",
       "2  ['당사는 선박투자회사법에 따라 케이에스에프선박금융 가 발기인이 되어 설립되었으며 ...  \n",
       "3  ['지회사 (持    Holding Company) 란 다른 회사의 식을 소유한 회...  \n",
       "4  ['가  산업개황 및 전망 당사의 매출과 이익은 중국 내에서의 건강식품 생산 및 판...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다트 데이터 받아오기\n",
    "dataset = datasets.load_dataset(\"nlpHakdang/beneficiary\", data_files = \"dart_v3_01.csv\", use_auth_token= \"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\")\n",
    "dataset\n",
    "dart_df = pd.DataFrame( dataset['train'] )\n",
    "del dart_df['회사의 개요']\n",
    "del dart_df['회사의 연혁']\n",
    "del dart_df['주요 제품 및 서비스']\n",
    "del dart_df['주요계약 및 연구개발활동']\n",
    "dart_df.head()\n",
    "# df.to_csv(\"dart_ret_v0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa774ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2daad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 데이터만 선별\n",
    "# news_df = pd.read_csv(\"news_train_ret_v0.csv\")\n",
    "news_data = list( new_tr_df['new_article'] )[:10]\n",
    "\n",
    "# dart_df = pd.read_csv(\"dart_ret_v0.csv\")\n",
    "dart_data = dart_df['사업의 개요'].map(eval)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060274e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d25a292-196e-42b2-9109-9388aa1d5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    # AdamW, get_linear_schedule_with_warmup,\n",
    "    # TrainingArguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783bf16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0da3479-7bc2-4ed1-8ee3-6235903977ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ae4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 원형 만들기\n",
    "# 기업과 뉴스 pair가 만들어져 있다고 가정.  여기서 0번째 두 데이터가 페어라고 가정.\n",
    " \n",
    "for psg in dart_data[0]:\n",
    "    # print( ( news_data[0], psg ) )\n",
    "    ( news_data[0], psg )\n",
    "    break\n",
    "\n",
    "# 데이터 셋에 넣을 때 tensor 형태로 넣을 수 있다.\n",
    "# 각 ids, token, type 텐서로 쪼개진다.\n",
    "# 뉴스는 (3, len_seq)\n",
    "# 다트는 (k, 3, len_seq)\n",
    "\n",
    "news_tok = tokenizer(news_data,padding = \"max_length\", return_tensors = \"pt\", truncation = True) # string 1개\n",
    "\n",
    "news_input_ids = news_tok['input_ids']\n",
    "news_type_ids = news_tok['token_type_ids']\n",
    "news_att_mask = news_tok['attention_mask']\n",
    "torch.stack( (news_att_mask, news_att_mask, news_att_mask), dim = 0 ).shape\n",
    "# error -> tokenized list is None...\n",
    "# from collections import defaultdict\n",
    "# dic = defaultdict(int)\n",
    "# for d in dart_data:\n",
    "#     dic[len(d)] += 1\n",
    "#     if len(d) == 0:\n",
    "#         print(d)\n",
    "# sorted(dic.items(), key = lambda x:x[0])\n",
    "\n",
    "\n",
    "# 다트 데이터\n",
    "dart_input_ids = []\n",
    "dart_type_ids = []\n",
    "dart_att_mask = []\n",
    "\n",
    "psg_table = []\n",
    "max_num_psg = 0\n",
    "for d in dart_data:\n",
    "    if len(d) > 0:\n",
    "        d_tok = tokenizer(d, padding = \"max_length\", return_tensors = \"pt\", truncation = True ) # List[str]\n",
    "        dart_input_ids.append( d_tok['input_ids'] )\n",
    "        dart_type_ids.append( d_tok['token_type_ids'] )\n",
    "        dart_att_mask.append( d_tok['attention_mask'] )\n",
    "\n",
    "        num_psg = d_tok['attention_mask'].shape[0]\n",
    "        max_num_psg = max(num_psg, max_num_psg)\n",
    "        psg_table.append( num_psg )\n",
    "psg_table = torch.tensor(psg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d48dbe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 차원 맞추기 위해서 채워넣어야 한다. 0 텐서?\n",
    "# 1개로 test\n",
    "ids = dart_input_ids[0]\n",
    "ids.shape\n",
    "seq_len = ids.shape[1]\n",
    "data_size = ids.shape[0]\n",
    "padding = torch.zeros( max_num_psg - data_size, seq_len )\n",
    "res = torch.cat((ids, padding))\n",
    "\n",
    "\n",
    "# check all value maintained and same\n",
    "ids == res[:data_size]\n",
    "\n",
    "\n",
    "padding.shape, ids.shape, res.shape, res[:data_size].shape#, ids, res[:data_size]\n",
    "res.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74fa39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 88, 512]), torch.Size([10, 512]), torch.Size([10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 다트 데이터에 대해서 passing 채워넣기\n",
    "input_ids_list_same_size = []\n",
    "type_ids_list_same_size = []\n",
    "att_mask_list_same_size = []\n",
    "for i in range(len(dart_input_ids)):\n",
    "    ids = dart_input_ids[i]\n",
    "    ids.shape\n",
    "    seq_len = ids.shape[1]\n",
    "    data_size = ids.shape[0]\n",
    "    padding = torch.zeros( max_num_psg - data_size, seq_len )\n",
    "    \n",
    "    res = torch.cat((dart_input_ids[i], padding))\n",
    "    input_ids_list_same_size.append( res )\n",
    "\n",
    "    res = torch.cat((dart_type_ids[i], padding))\n",
    "    type_ids_list_same_size.append( res )\n",
    "\n",
    "    res = torch.cat((dart_att_mask[i], padding))\n",
    "    att_mask_list_same_size.append( res )\n",
    "\n",
    "\n",
    "dart_input_ids = torch.stack(input_ids_list_same_size)\n",
    "dart_type_ids = torch.stack(type_ids_list_same_size)\n",
    "dart_att_mask = torch.stack(att_mask_list_same_size)\n",
    "\n",
    "dart_att_mask.shape, news_att_mask.shape, psg_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb17655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa4ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(news_input_ids, news_type_ids, news_att_mask, dart_input_ids, dart_type_ids, dart_att_mask,  psg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb928779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a356bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fee39011bb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "dataloader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb2e77",
   "metadata": {},
   "source": [
    "# Cumstom Dataset으로 시도 했으나 여기도 어차피 stack으로 배치를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c6203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dustom dataset으로 시도\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class Train_Dataset(Dataset):\n",
    "\n",
    "#     def __init__(self): \n",
    "#     # def __init__(self, df, source_column, target_column, transform=None, freq_threshold = 5,\n",
    "#     #             source_vocab_max_size = 10000, target_vocab_max_size = 10000):\n",
    "#         news_tok = tokenizer(news_data,padding = \"max_length\", return_tensors = \"pt\", truncation = True) # string 1개\n",
    "\n",
    "#         self.news_input_ids = news_tok['input_ids']\n",
    "#         self.news_type_ids = news_tok['token_type_ids']\n",
    "#         self.news_att_mask = news_tok['attention_mask']\n",
    "\n",
    "#         self.dart_input_ids = []\n",
    "#         self.dart_type_ids = []\n",
    "#         self.dart_att_mask = []\n",
    "\n",
    "#         psg_table = []\n",
    "#         max_num_psg = 0\n",
    "#         for d in dart_data:\n",
    "#             if len(d) > 0:\n",
    "#                 d_tok = tokenizer(d, padding = \"max_length\", return_tensors = \"pt\", truncation = True ) # List[str]\n",
    "#                 self.dart_input_ids.append( d_tok['input_ids'] )\n",
    "#                 self.dart_type_ids.append( d_tok['token_type_ids'] )\n",
    "#                 self.dart_att_mask.append( d_tok['attention_mask'] )\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.news_input_ids.shape[0]#len()\n",
    "    \n",
    "#     '''\n",
    "#     __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
    "#     target values using the vocabulary objects we created in __init__\n",
    "#     '''\n",
    "#     def __getitem__(self, index):\n",
    "#         # self.\n",
    "#         news_source = torch.stack( (self.news_att_mask[index], self.news_att_mask[index], self.news_att_mask[index]), dim = 0 )#.shape\n",
    "#         dart_source = torch.stack( (self.dart_att_mask[index], self.dart_att_mask[index], self.dart_att_mask[index]), dim = 0 )#.shape\n",
    "\n",
    "\n",
    "#         # source_text = self.source_texts[index]\n",
    "#         # target_text = self.target_texts[index]\n",
    "        \n",
    "#         # if self.transform is not None:\n",
    "#         #     source_text = self.transform(source_text)\n",
    "            \n",
    "#         # #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
    "#         # numerialized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "#         # numerialized_source += self.source_vocab.numericalize(source_text)\n",
    "#         # numerialized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "    \n",
    "#         # numerialized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "#         # numerialized_target += self.target_vocab.numericalize(target_text)\n",
    "#         # numerialized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "#         #convert the list to tensor and return\n",
    "#         # return torch.tensor(numerialized_source), torch.tensor(numerialized_target) \n",
    "#         return torch.tensor(news_source), torch.tensor(dart_source) \n",
    "# train_set = Train_Dataset()\n",
    "# BATCH_SIZE = 2\n",
    "# dataloader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26195a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee3c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TensorDataset(news_tok['input_ids'],news_tok['token_type_ids'],news_tok['attention_mask']\\\n",
    "    # )\n",
    "# dataset = TensorDataset(news_input_ids.T, dart_input_ids.T)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3772f",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ca2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "      \n",
    "    def forward(self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1bd93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee1d13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertEncoder.from_pretrained(\"klue/bert-base\").to(device)\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c7217",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4d97dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (88) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [88, 512].  Tensor sizes: [3, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26117/3645943393.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# ids[b] = ids[b][:psg_table[b]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpsg_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m# print(ids[b].shape, psg_table[b], ids[b][:psg_table[b]].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (88) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [88, 512].  Tensor sizes: [3, 512]"
     ]
    }
   ],
   "source": [
    "# for given batch with batch size B\n",
    "for batch in dataloader:\n",
    "    # print(batch[0].shape)\n",
    "    # print(batch[1].shape)\n",
    "    # print(batch[2].shape)\n",
    "    news_input_ids = news_tok['input_ids']\n",
    "    news_type_ids = news_tok['token_type_ids']\n",
    "    news_att_mask = news_tok['attention_mask']\n",
    "\n",
    "    news_data = {\n",
    "        'input_ids' : batch[0],\n",
    "        'token_type_ids' : batch[1],\n",
    "        'attention_mask' : batch[2],\n",
    "    }\n",
    "\n",
    "    # dart_data = {\n",
    "    #     'input_ids' : batch[3],\n",
    "    #     'token_type_ids' : batch[4],\n",
    "    #     'attention_mask' : batch[5],\n",
    "    # }\n",
    "\n",
    "    psg_table = batch[6] # ( B )\n",
    "\n",
    "    for batch_idx in range(3,6):\n",
    "        ids = batch[batch_idx] # ( B, max_psg, seq_len ) # input_ids, type_ids, att_mask\n",
    "\n",
    "\n",
    "        for b in range(BATCH_SIZE): \n",
    "            # ids[b] = ids[b][:psg_table[b]]\n",
    "            ids[b] = ids[b][:psg_table[b]]\n",
    "            # print(ids[b].shape, psg_table[b], ids[b][:psg_table[b]].shape)\n",
    "\n",
    "\n",
    "        # for data_size in psg_table:\n",
    "            # batch[batch_idx] = batch[batch_idx][:data_size]\n",
    "\n",
    "    print(psg_table)\n",
    "    print(batch[0].shape, batch[3].shape)\n",
    "\n",
    "#     batch <- ( news_data, dart_data )\n",
    "#     # news_data # (B, 1, hidden_dim, seq_len)\n",
    "#     # dart_data # (B, k_j', hidden_dim, seq_len)\n",
    "\n",
    "\n",
    "#     # must change dim before input here to ( B, hidden_dim, seq_len )\n",
    "#     # encoder output use CLS token at first. No reason. Just convinience. \n",
    "#     news_vec = news_encoder(news_data)  # ( B, hidden_dim, 1) \n",
    "#     change dim from (B, hidden_dim, 1) => (B, 1, hidden_dim) \n",
    "\n",
    "#     # must change dim before input here to (B * k_j', hidden_dim, seq_len )\n",
    "#     # encoder output use CLS token at first. No reason. Just convinience. \n",
    "#     dart_vec = dart_encoder(dart_data)  # (B * k_j', hidden_dim, 1)\n",
    "#     change dim from  (B * k_j', hidden_dim, 1) => (B, k_j', hidden_dim, 1) => (B, k_j', hidden_dim)\n",
    "\n",
    "#     # calculate similiarity\n",
    "#     # tensor의 multiplication은 뒤 두개는 내적하고 그 앞은 element wise한다는 성질을 적용할 수만 있다면...\n",
    "#     sim_score = news_vec * dart_vec.T # ( B, 1, hidden_dim ) * ( B, hidden_dim, k_j')  => (B * B, 1, k_j')\n",
    "#     change dim from (B * B, 1, k_j') => ( B * B, k_j')\n",
    "\n",
    "#     # 동일 기업에 대해 하나로 만들어야 한다.\n",
    "#     # soft voting 혹은 평균으로 실수화. 혹은 max. 이것도 나름의 pooling 인것 같다...\n",
    "#     # 버트에서 나올 때 pooling, 여기서 또 pooling... 잘 될까...? ㅜㅜ\n",
    "#     sim_score_per_corp = some_func(sim_score)  # (B * B, 1) \n",
    "#     change dim from (B * B, 1) => (B * B)\n",
    "\n",
    "#     # ground truth는 대각행렬을 구하면 됨.\n",
    "#     # one_B = torch.ones(B) # ( B )\n",
    "#     # target = torch.diag( one_B ) # 2D diagonal matrix with value 1 size ( B, B )\n",
    "#     # 하지만 언제나 cross entropy가 이걸 알아서 해주지만 항상 까먹지 히히 \n",
    "#     target = torch.arrange(B)\n",
    "\n",
    "#     # calc loss\n",
    "#     # B개에서 각 sample 마다 news와 유사한 dart 기업을 선택해야 함. \n",
    "#     loss = cross_entropy_loss( sim_score_per_corp, target) \n",
    "\n",
    "#     loss.backward()\n",
    "#     ...\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
