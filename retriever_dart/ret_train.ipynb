{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f47ec80-e266-41ae-ba64-84b3dcb3fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27ad9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수들\n",
    "from dart_prcs import *\n",
    "\n",
    "\n",
    "def stringfy(tmp):\n",
    "    tmp = eval(tmp) \n",
    "    return \" \".join( tmp )\n",
    "\n",
    "def preprocess(text):\n",
    "    import regex as re\n",
    "    text = remove_repeated_spacing(text)\n",
    "    text = clean_punc(text)\n",
    "    text = remove_useless_breacket(text)\n",
    "    text = remove_email(text)\n",
    "    text = remove_url(text)\n",
    "    text = re.sub(\"\\n\", \"\", text) \n",
    "    text = re.sub(\"\\\"\", \"\", text)\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    text = re.sub(\"-\", \"\", text)\n",
    "    text = re.sub(\"\\\"\", \"\", text)\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    text = re.sub(\",\", \"\", text)\n",
    "    text = re.sub(\"\\.\", \"\", text)\n",
    "    text = re.sub(\"(주)\", \"\", text)\n",
    "    text = re.sub(\"\\)\", \" \", text)\n",
    "    text = re.sub(\"\\(\", \" \", text)\n",
    "    \n",
    "    # text = re.sub(\"[(]\", \" (\", text)\n",
    "    # text = re.sub(\"[)]\", \") \", text)\n",
    "    text = re.sub('[■:%/~㈜↓&※·→①②$③○-ㆍ」「■>Ⅱ④;▶●?⑤社⑥⑦□=ㅇ『』外<◆△【】現▲▷美∼用☞@前㎡◇中Ⅲ－無新內％◈}株ㅁ會㎥{ㄱⅠ化高＋ㄴ日有：ㄷ公司全後限，〔〕學↑式月|＆ℓㄹ…業人名《》年^韓部▼本大小海│愛故食形㎏獨山多水東可非思州國家生上℃ㅂ合金發在同⊙軍英物實田開○○○○○作性體度産空分子光重ㅅ島間時利＂面母≪㎖資心別氣仁未京來對成雲淸聖命保的集史靑場法神正第一硏㎞ㅡ★太民如理出入下解得安平＝帝所市石門相方元政先富北木自車南地―｜求ㅎㅎ≫西長銀者規制女江福和ㅌ通主義村當代力㎝善原選色；古河──都能動歌〈〉不定ㅠ吉事理張數朝金㎜記書]'\\\n",
    "                      , \" \", text)\n",
    "    text = remove_repeated_spacing(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def prcs_news_dataset(dataset):\n",
    "    news_df = pd.DataFrame( {'title' : dataset['title'], 'original' : dataset['original'], 'category':dataset['category'],'article' : dataset['article'] } )\n",
    "    eco_df = news_df[ news_df['category'] == \"경제\" ]\n",
    "    eco_df['new_article'] = eco_df['original'].map(stringfy)\n",
    "    eco_df['new_article'] = eco_df['new_article'].map(preprocess)\n",
    "    return eco_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9a076ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 뉴스 데이터 받아오기\n",
    "# news_dataset = datasets.load_dataset('nlpHakdang/aihub-news30k',  data_files={\"train\":\"news_train_1_1.csv\", \"valid\":\"news_valid_1_1.csv\"}, use_auth_token='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')\n",
    "\n",
    "# news_train_set = news_dataset['train']\n",
    "# news_val_set = news_dataset['valid']\n",
    "\n",
    "\n",
    "\n",
    "# new_tr_df = prcs_news_dataset( news_train_set )\n",
    "# new_vl_df = prcs_news_dataset( news_val_set )\n",
    "\n",
    "# # see\n",
    "# # r = np.random.randint(len(new_tr_df))\n",
    "# # new_tr_df.iloc[r]['new_article'],new_tr_df.iloc[r]['original']\n",
    "\n",
    "# del new_tr_df['original']\n",
    "# del new_tr_df['article']\n",
    "# del new_vl_df['original']\n",
    "# del new_vl_df['article']\n",
    "\n",
    "# # new_tr_df.to_csv(\"news_train_ret_v0.csv\")\n",
    "# # new_vl_df.to_csv(\"news_valid_ret_v0.csv\")\n",
    "new_tr_df = pd.read_csv(\"news_train_ret_v0.csv\")\n",
    "new_vl_df = pd.read_csv(\"news_valid_ret_v0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf1b6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 다트 데이터 받아오기\n",
    "# dataset = datasets.load_dataset(\"nlpHakdang/beneficiary\", data_files = \"dart_v3_01.csv\", use_auth_token= \"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\")\n",
    "# dataset\n",
    "# dart_df = pd.DataFrame( dataset['train'] )\n",
    "# del dart_df['회사의 개요']\n",
    "# del dart_df['회사의 연혁']\n",
    "# del dart_df['주요 제품 및 서비스']\n",
    "# del dart_df['주요계약 및 연구개발활동']\n",
    "# dart_df.head()\n",
    "# # dart_df.to_csv(\"dart_ret_v0.csv\")\n",
    "dart_df = pd.read_csv(\"dart_ret_v0.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa774ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2daad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 데이터만 선별\n",
    "# news_df = pd.read_csv(\"news_train_ret_v0.csv\")\n",
    "news_data = list( new_tr_df['new_article'] )[:10]\n",
    "\n",
    "# dart_df = pd.read_csv(\"dart_ret_v0.csv\")\n",
    "dart_data = dart_df['사업의 개요'].map(eval)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060274e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d25a292-196e-42b2-9109-9388aa1d5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    # AdamW, get_linear_schedule_with_warmup,\n",
    "    # TrainingArguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "783bf16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0da3479-7bc2-4ed1-8ee3-6235903977ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04ae4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 원형 만들기\n",
    "# 기업과 뉴스 pair가 만들어져 있다고 가정.  여기서 0번째 두 데이터가 페어라고 가정.\n",
    " \n",
    "for psg in dart_data[0]:\n",
    "    # print( ( news_data[0], psg ) )\n",
    "    ( news_data[0], psg )\n",
    "    break\n",
    "\n",
    "# 데이터 셋에 넣을 때 tensor 형태로 넣을 수 있다.\n",
    "# 각 ids, token, type 텐서로 쪼개진다.\n",
    "# 뉴스는 (3, len_seq)\n",
    "# 다트는 (k, 3, len_seq)\n",
    "\n",
    "news_tok = tokenizer(news_data,padding = \"max_length\", return_tensors = \"pt\", truncation = True) # string 1개\n",
    "\n",
    "news_input_ids = news_tok['input_ids']\n",
    "news_type_ids = news_tok['token_type_ids']\n",
    "news_att_mask = news_tok['attention_mask']\n",
    "torch.stack( (news_att_mask, news_att_mask, news_att_mask), dim = 0 ).shape\n",
    "# error -> tokenized list is None...\n",
    "# from collections import defaultdict\n",
    "# dic = defaultdict(int)\n",
    "# for d in dart_data:\n",
    "#     dic[len(d)] += 1\n",
    "#     if len(d) == 0:\n",
    "#         print(d)\n",
    "# sorted(dic.items(), key = lambda x:x[0])\n",
    "\n",
    "\n",
    "# 다트 데이터\n",
    "dart_input_ids = []\n",
    "dart_type_ids = []\n",
    "dart_att_mask = []\n",
    "\n",
    "psg_table = []\n",
    "max_num_psg = 0\n",
    "for d in dart_data:\n",
    "    if len(d) > 0:\n",
    "        d_tok = tokenizer(d, padding = \"max_length\", return_tensors = \"pt\", truncation = True ) # List[str]\n",
    "        dart_input_ids.append( d_tok['input_ids'] )\n",
    "        dart_type_ids.append( d_tok['token_type_ids'] )\n",
    "        dart_att_mask.append( d_tok['attention_mask'] )\n",
    "\n",
    "        num_psg = d_tok['attention_mask'].shape[0]\n",
    "        max_num_psg = max(num_psg, max_num_psg)\n",
    "        psg_table.append( num_psg )\n",
    "psg_table = torch.tensor(psg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d48dbe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 차원 맞추기 위해서 채워넣어야 한다. 0 텐서?\n",
    "# 1개로 test\n",
    "ids = dart_input_ids[0]\n",
    "ids.shape\n",
    "seq_len = ids.shape[1]\n",
    "data_size = ids.shape[0]\n",
    "padding = torch.zeros( max_num_psg - data_size, seq_len )\n",
    "res = torch.cat((ids, padding))\n",
    "\n",
    "\n",
    "# check all value maintained and same\n",
    "ids == res[:data_size]\n",
    "\n",
    "\n",
    "padding.shape, ids.shape, res.shape, res[:data_size].shape#, ids, res[:data_size]\n",
    "res.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b74fa39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 88, 512]), torch.Size([10, 512]), torch.Size([10]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 다트 데이터에 대해서 passing 채워넣기\n",
    "input_ids_list_same_size = []\n",
    "type_ids_list_same_size = []\n",
    "att_mask_list_same_size = []\n",
    "for i in range(len(dart_input_ids)):\n",
    "    ids = dart_input_ids[i]\n",
    "    ids.shape\n",
    "    seq_len = ids.shape[1]\n",
    "    data_size = ids.shape[0]\n",
    "    padding = torch.zeros( max_num_psg - data_size, seq_len )\n",
    "    \n",
    "    res = torch.cat((dart_input_ids[i], padding))\n",
    "    input_ids_list_same_size.append( res )\n",
    "\n",
    "    res = torch.cat((dart_type_ids[i], padding))\n",
    "    type_ids_list_same_size.append( res )\n",
    "\n",
    "    res = torch.cat((dart_att_mask[i], padding))\n",
    "    att_mask_list_same_size.append( res )\n",
    "\n",
    "\n",
    "dart_input_ids = torch.stack(input_ids_list_same_size)\n",
    "dart_type_ids = torch.stack(type_ids_list_same_size)\n",
    "dart_att_mask = torch.stack(att_mask_list_same_size)\n",
    "\n",
    "dart_att_mask.shape, news_att_mask.shape, psg_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78eec46d-5dc3-4736-b58c-4449a849377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view 테스트\n",
    "# dart_att_mask.view(10 * 88, -1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb17655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfa4ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(news_input_ids, news_type_ids, news_att_mask, dart_input_ids, dart_type_ids, dart_att_mask,  psg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb928779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8a356bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f64032279d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "dataloader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb2e77",
   "metadata": {},
   "source": [
    "# Cumstom Dataset으로 시도 했으나 여기도 어차피 stack으로 배치를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37c6203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dustom dataset으로 시도\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class Train_Dataset(Dataset):\n",
    "\n",
    "#     def __init__(self): \n",
    "#     # def __init__(self, df, source_column, target_column, transform=None, freq_threshold = 5,\n",
    "#     #             source_vocab_max_size = 10000, target_vocab_max_size = 10000):\n",
    "#         news_tok = tokenizer(news_data,padding = \"max_length\", return_tensors = \"pt\", truncation = True) # string 1개\n",
    "\n",
    "#         self.news_input_ids = news_tok['input_ids']\n",
    "#         self.news_type_ids = news_tok['token_type_ids']\n",
    "#         self.news_att_mask = news_tok['attention_mask']\n",
    "\n",
    "#         self.dart_input_ids = []\n",
    "#         self.dart_type_ids = []\n",
    "#         self.dart_att_mask = []\n",
    "\n",
    "#         psg_table = []\n",
    "#         max_num_psg = 0\n",
    "#         for d in dart_data:\n",
    "#             if len(d) > 0:\n",
    "#                 d_tok = tokenizer(d, padding = \"max_length\", return_tensors = \"pt\", truncation = True ) # List[str]\n",
    "#                 self.dart_input_ids.append( d_tok['input_ids'] )\n",
    "#                 self.dart_type_ids.append( d_tok['token_type_ids'] )\n",
    "#                 self.dart_att_mask.append( d_tok['attention_mask'] )\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.news_input_ids.shape[0]#len()\n",
    "    \n",
    "#     '''\n",
    "#     __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
    "#     target values using the vocabulary objects we created in __init__\n",
    "#     '''\n",
    "#     def __getitem__(self, index):\n",
    "#         # self.\n",
    "#         news_source = torch.stack( (self.news_att_mask[index], self.news_att_mask[index], self.news_att_mask[index]), dim = 0 )#.shape\n",
    "#         dart_source = torch.stack( (self.dart_att_mask[index], self.dart_att_mask[index], self.dart_att_mask[index]), dim = 0 )#.shape\n",
    "\n",
    "\n",
    "#         # source_text = self.source_texts[index]\n",
    "#         # target_text = self.target_texts[index]\n",
    "        \n",
    "#         # if self.transform is not None:\n",
    "#         #     source_text = self.transform(source_text)\n",
    "            \n",
    "#         # #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
    "#         # numerialized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "#         # numerialized_source += self.source_vocab.numericalize(source_text)\n",
    "#         # numerialized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "    \n",
    "#         # numerialized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "#         # numerialized_target += self.target_vocab.numericalize(target_text)\n",
    "#         # numerialized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "#         #convert the list to tensor and return\n",
    "#         # return torch.tensor(numerialized_source), torch.tensor(numerialized_target) \n",
    "#         return torch.tensor(news_source), torch.tensor(dart_source) \n",
    "# train_set = Train_Dataset()\n",
    "# BATCH_SIZE = 2\n",
    "# dataloader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26195a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bee3c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TensorDataset(news_tok['input_ids'],news_tok['token_type_ids'],news_tok['attention_mask']\\\n",
    "    # )\n",
    "# dataset = TensorDataset(news_input_ids.T, dart_input_ids.T)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3772f",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63ca2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "      \n",
    "    def forward(self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f1bd93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ee1d13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertEncoder.from_pretrained(\"klue/bert-base\").to(device)\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c7217",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4d97dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for given batch with batch size B\n",
    "\n",
    "\n",
    "#     batch <- ( news_data, dart_data )\n",
    "#     # news_data # (B, 1, hidden_dim, seq_len)\n",
    "#     # dart_data # (B, k_j', hidden_dim, seq_len)\n",
    "\n",
    "\n",
    "#     # must change dim before input here to ( B, hidden_dim, seq_len )\n",
    "#     # encoder output use CLS token at first. No reason. Just convinience. \n",
    "#     news_vec = news_encoder(news_data)  # ( B, hidden_dim, 1) \n",
    "#     change dim from (B, hidden_dim, 1) => (B, 1, hidden_dim) \n",
    "\n",
    "#     # must change dim before input here to (B * k_j', hidden_dim, seq_len )\n",
    "#     # encoder output use CLS token at first. No reason. Just convinience. \n",
    "#     dart_vec = dart_encoder(dart_data)  # (B * k_j', hidden_dim, 1)\n",
    "#     change dim from  (B * k_j', hidden_dim, 1) => (B, k_j', hidden_dim, 1) => (B, k_j', hidden_dim)\n",
    "\n",
    "#     # calculate similiarity\n",
    "#     # tensor의 multiplication은 뒤 두개는 내적하고 그 앞은 element wise한다는 성질을 적용할 수만 있다면...\n",
    "#     sim_score = news_vec * dart_vec.T # ( B, 1, hidden_dim ) * ( B, hidden_dim, k_j')  => (B * B, 1, k_j')\n",
    "#     change dim from (B * B, 1, k_j') => ( B * B, k_j')\n",
    "\n",
    "#     # 동일 기업에 대해 하나로 만들어야 한다.\n",
    "#     # soft voting 혹은 평균으로 실수화. 혹은 max. 이것도 나름의 pooling 인것 같다...\n",
    "#     # 버트에서 나올 때 pooling, 여기서 또 pooling... 잘 될까...? ㅜㅜ\n",
    "#     sim_score_per_corp = some_func(sim_score)  # (B * B, 1) \n",
    "#     change dim from (B * B, 1) => (B * B)\n",
    "\n",
    "#     # ground truth는 대각행렬을 구하면 됨.\n",
    "#     # one_B = torch.ones(B) # ( B )\n",
    "#     # target = torch.diag( one_B ) # 2D diagonal matrix with value 1 size ( B, B )\n",
    "#     # 하지만 언제나 cross entropy가 이걸 알아서 해주지만 항상 까먹지 히히 \n",
    "#     target = torch.arrange(B)\n",
    "\n",
    "#     # calc loss\n",
    "#     # B개에서 각 sample 마다 news와 유사한 dart 기업을 선택해야 함. \n",
    "#     loss = cross_entropy_loss( sim_score_per_corp, target) \n",
    "\n",
    "#     loss.backward()\n",
    "#     ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d481256c-0a58-4297-8331-8e9301f7cd4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 31.75 GiB total capacity; 30.36 GiB already allocated; 176.75 MiB free; 30.45 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3268/1299470631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdart_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3268/2205670248.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     14\u001b[0m         ): \n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 31.75 GiB total capacity; 30.36 GiB already allocated; 176.75 MiB free; 30.45 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "\n",
    "\n",
    "    news_data = {\n",
    "        'input_ids' : batch[0].to(device).to(torch.int64), # ( B, seq_len )\n",
    "        'token_type_ids' : batch[1].to(device).to(torch.int64),\n",
    "        'attention_mask' : batch[2].to(device).to(torch.int64),\n",
    "    }\n",
    "\n",
    "    \n",
    "    B = batch[3].shape[0]\n",
    "    max_psg_len =  batch[3].shape[1]\n",
    "    \n",
    "    dart_data = {\n",
    "        'input_ids' : batch[3].view(B * max_psg_len, -1).to(device).to(torch.int64), # (B, max_psg_len, seq_len)\n",
    "        'token_type_ids' : batch[4].view(B * max_psg_len, -1).to(device).to(torch.int64),\n",
    "        'attention_mask' : batch[5].view(B * max_psg_len, -1).to(device).to(torch.int64),\n",
    "    }\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    model(**dart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591399e-d304-4064-bcd0-6045cd92bfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
