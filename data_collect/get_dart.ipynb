{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules and functions\n",
    "# 1. 전체 기업 정보 불러오기 -> 파일럿 데이터: stock 있는 기업만 따로 분류하기(parse2)\n",
    "# 2. 기업들 최근 공시 불러오기 -> 최근 문서 번호 가지고 오기(parse1)\n",
    "# 3. 최근 문서 불러오기 -> 파싱하기(parse2)\n",
    "# 모듈\n",
    "\n",
    "# request 모듈. argument(url, params). 특이사항: josn과 xml이 다르다. json은 josn으로 xml은 io byte으로 변환해야 한다\n",
    "\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import time\n",
    "import tqdm.notebook as tq\n",
    "import pickle\n",
    "from pathos.pools import ProcessPool\n",
    "\n",
    "api_key = 'd81e78aa719d1c1e4ec7867ef22a737ab6cbb4c7' # 어디서 받아온 거 ㅎㅎ\n",
    "\n",
    "def init_bs4(xml_str):\n",
    "    return BeautifulSoup(xml_str, \"lxml\")\n",
    "\n",
    "\n",
    "def xml_parser(response):\n",
    "    zf = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    info_list = zf.infolist()\n",
    "    xml_data = zf.read(info_list[0].filename)\n",
    "    try:\n",
    "        xml_text = xml_data.decode('euc-kr')\n",
    "        # print(\"xml paring case 1\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        xml_text = xml_data.decode('utf-8')\n",
    "        # print(\"xml paring case 2\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        # print(\"xml paring case #\")\n",
    "        xml_text = xml_data\n",
    "    return init_bs4(xml_text)\n",
    "\n",
    "def request_dart(url, params):\n",
    "    r = requests.get(url, params=params)\n",
    "    return r\n",
    "\n",
    "def create_params(keys = None, values= None):\n",
    "    params = {\n",
    "        'crtfc_key': api_key,\n",
    "    }\n",
    "\n",
    "    if keys:\n",
    "        assert len(keys) == len(values), \"key and value lengths must be same.\"\n",
    "        for i in range(len(keys)):\n",
    "            params[keys[i]] = values[i]\n",
    "\n",
    "    return params\n",
    "\n",
    "def request_dart_corps_info():\n",
    "    url = 'https://opendart.fss.or.kr/api/corpCode.xml'\n",
    "    params = create_params()\n",
    "    res = request_dart(url, params)\n",
    "    return res\n",
    "    \n",
    "\n",
    "def get_corp(only_stock = False):\n",
    "    res = request_dart_corps_info()\n",
    "\n",
    "    parser = xml_parser(res)\n",
    "    corps_xml = parser.find_all(\"list\")\n",
    "\n",
    "    corps = []\n",
    "    num_stock = 0\n",
    "\n",
    "    for l in corps_xml:\n",
    "        corp_dict = {}\n",
    "        stock = None\n",
    "        if l.stock_code.string != \" \":\n",
    "            stock = l.stock_code.string\n",
    "            num_stock += 1\n",
    "\n",
    "        if ( only_stock == True and stock ) or only_stock == False:\n",
    "            corp_dict[\"주식 코드\"] = str(stock)\n",
    "            corp_dict[\"기업 코드\"] = str(l.corp_code.string)\n",
    "            corp_dict[\"기업 이름\"] = str(l.corp_name.string)\n",
    "            \n",
    "            corp_dict[\"수정 일자\"] = str(l.modify_date.string)\n",
    "            corps.append( corp_dict )\n",
    "    return corps\n",
    "\n",
    "\n",
    "def get_corp_code(only_stock = False):\n",
    "    corps = get_corp(only_stock)\n",
    "    corp_code = []\n",
    "    for c in corps:\n",
    "        corp_code.append(c['기업 코드'])\n",
    "    return corp_code\n",
    "\n",
    "\n",
    "def get_report_index(corps):\n",
    "    keys = ['bgn_de', 'pblntf_ty']\n",
    "    values = ['20210101', 'A']\n",
    "\n",
    "    params = create_params(keys, values)\n",
    "\n",
    "    active_corps = []\n",
    "    \n",
    "    print(len(corps))\n",
    "    for i, corp in tq.tqdm(enumerate(corps)):\n",
    "        act_corp = {}\n",
    "\n",
    "        code = corp['기업 코드']\n",
    "        params['corp_code'] = code\n",
    "\n",
    "        url = 'https://opendart.fss.or.kr/api/list.json'\n",
    "\n",
    "        res = request_dart(url, params)\n",
    "        res = res.json()\n",
    "        if res['status'] == \"000\":\n",
    "            latest_doc = res['list'][0]\n",
    "            for k, v in corp.items():\n",
    "                act_corp[k] = v\n",
    "                act_corp['report_idx'] = latest_doc['rcept_no']\n",
    "            active_corps.append( act_corp )\n",
    "        else:\n",
    "            print(f\"{res['status']}: {res['message']}: {corp} \")\n",
    "\n",
    "        LIMIT = 800\n",
    "        if i > 0 and i % LIMIT == 0:\n",
    "            print(f\"so far {len(active_corps)} has been collected...\")\n",
    "            # debug\n",
    "            # if len(active_corps) > 0:\n",
    "            #   return active_corps\n",
    "            print(f\"{LIMIT} requests has done. sleep 60 secs...\")\n",
    "\n",
    "            time.sleep(60)\n",
    "            \n",
    "\n",
    "          \n",
    "    return active_corps\n",
    "\n",
    "def parse_xml(soup):\n",
    "    contents = {}\n",
    "    for s in soup.find_all(\"section-1\"):\n",
    "        title = str(s.title.string)\n",
    "        sub_chapters = {}\n",
    "        for ss in s.find_all(\"section-2\"):\n",
    "            sub_title = str(ss.title.string)\n",
    "\n",
    "            clean_text = str(ss.get_text())\n",
    "            sub_chapters[sub_title] = clean_text\n",
    "        contents[title] = sub_chapters\n",
    "    return contents\n",
    "\n",
    "def remove_table(soup):\n",
    "    for t in soup.find_all('table'):\n",
    "        t.decompose()\n",
    "    assert len(soup.find_all('table')) == 0, \"table still exists!\"\n",
    "\n",
    "def get_report(corp):\n",
    "    r = corp['report_idx']\n",
    "    keys = ['rcept_no']\n",
    "    values = [r]\n",
    "\n",
    "    params = create_params(keys, values)\n",
    "\n",
    "    url = 'https://opendart.fss.or.kr/api/document.xml'\n",
    "    r = requests.get(url, params=params)\n",
    "    return r\n",
    "\n",
    "def get_doc(active_corps):\n",
    "\n",
    "    for i, c in tq.tqdm(enumerate(active_corps)):\n",
    "        r = get_report(c)\n",
    "        LIMIT = 800\n",
    "        if i > 0 and i % LIMIT == 0:\n",
    "            print(\"cur i = \", i)\n",
    "            # print(f\"so far {len(report_idx)} has been collected...\")\n",
    "            print(f\"{LIMIT} requests has done. sleep 60 secs...\")\n",
    "\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            soup = xml_parser(r)\n",
    "            c['original_xml'] = str(soup)\n",
    "            remove_table(soup)\n",
    "            contents = parse_xml(soup)\n",
    "            c['contents'] = contents\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(c)\n",
    "            c['original_xml'] = None\n",
    "            # c['contents'] = None\n",
    "    return active_corps\n",
    "\n",
    "def get_active_corp(c):\n",
    "      \n",
    "        r = c['report_idx']\n",
    "        keys = ['rcept_no']\n",
    "        values = [r]\n",
    "\n",
    "        params = create_params(keys, values)\n",
    "\n",
    "        url = 'https://opendart.fss.or.kr/api/document.xml'\n",
    "        r = requests.get(url, params=params)  \n",
    "        \n",
    "        try:\n",
    "            soup = xml_parser(r)\n",
    "            c['original_xml'] = str(soup)\n",
    "            remove_table(soup)\n",
    "            contents = parse_xml(soup)\n",
    "            c['contents'] = contents\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(c)\n",
    "            c['original_xml'] = None\n",
    "            c['contents'] = None\n",
    "        return c\n",
    "        \n",
    "def parallel_get_doc(active_corps):\n",
    "    assert isinstance(active_corps, dict)\n",
    "    active_corps = get_active_corp(active_corps)\n",
    "    return active_corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업 정보\n",
    "corps = get_corp(only_stock = True)\n",
    "with open(\"all_corp_list.pickle\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(corps, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copr information -> get latest report id\n",
    "active_corps = get_report_index(corps)\n",
    "with open(\"active_corps.pickle\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(active_corps, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from latest report id -> get xml and parse them. \n",
    "# without multiprocess, it takes about 1 hour...\n",
    "pool = ProcessPool()\n",
    "LIMIT = 500\n",
    "content_orig_total = []\n",
    "for i in tq.tqdm(range(0, len(active_corps), LIMIT)):\n",
    "    print(f\"from {i} to {i+LIMIT}\")\n",
    "    content_orig = pool.map(parallel_get_doc, active_corps[i:i+LIMIT])\n",
    "    print(len(content_orig))\n",
    "    content_orig_total.extend(content_orig)\n",
    "    \n",
    "with open(\"content_orig_total.pickle\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(content_orig_total, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e31c68abf1d5dd3f9e2269f23eadf1b199587e56c0618a30760176a65ebfcab4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('lightweight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
