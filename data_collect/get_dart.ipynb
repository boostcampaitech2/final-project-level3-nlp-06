{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules and functions\n",
    "# 1. 전체 기업 정보 불러오기 -> 파일럿 데이터: stock 있는 기업만 따로 분류하기(parse2)\n",
    "# 2. 기업들 최근 공시 불러오기 -> 최근 문서 번호 가지고 오기(parse1)\n",
    "# 3. 최근 문서 불러오기 -> 파싱하기(parse2)\n",
    "# 모듈\n",
    "\n",
    "# request 모듈. argument(url, params). 특이사항: josn과 xml이 다르다. json은 josn으로 xml은 io byte으로 변환해야 한다\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import time\n",
    "import tqdm.notebook as tq\n",
    "import pickle\n",
    "from pathos.pools import ProcessPool\n",
    "import re\n",
    "\n",
    "# api_key = 'd81e78aa719d1c1e4ec7867ef22a737ab6cbb4c7' # 어디서 받아온 거 ㅎㅎ\n",
    "\n",
    "# api_key = 'ea372803285b0209349791434379d3fa748ae416' # 내꺼\n",
    "# api_key = 'c7ed49e31def2fbd7708c8e8684631dde89cb572' # 호영님꺼\n",
    "# api_key = '894583c60a20e9c904eb149616830888bfcc653d' # 요한님꺼\n",
    "\n",
    "def init_bs4(xml_str):\n",
    "    return BeautifulSoup(xml_str, \"lxml\")\n",
    "\n",
    "def generate_parser(response, hook):\n",
    "    \"\"\"\n",
    "        받아온 다트 데이터를 parsing할 generator입니다. xml을 파싱하기도 하고 json을 파싱하기도 해서 factory 패턴을 적용했습니다.\n",
    "    \"\"\"\n",
    "    zf = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    info_list = zf.infolist()\n",
    "    xml_data = zf.read(hook(info_list))\n",
    "\n",
    "    try:\n",
    "        xml_text = xml_data.decode('euc-kr')\n",
    "        # print(\"xml paring case 1\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        xml_text = xml_data.decode('utf-8')\n",
    "        # print(\"xml paring case 2\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        # print(\"xml paring case #\")\n",
    "        xml_text = xml_data\n",
    "    return init_bs4(xml_text)\n",
    "\n",
    "def create_get_corp_xml_parser(response):\n",
    "    def hook(info_list):\n",
    "        return info_list[0].filename\n",
    "    return generate_parser(response, hook)\n",
    "\n",
    "def create_report_xml_parser(response, report_idx):\n",
    "\n",
    "    def hook(info_list):\n",
    "        target_doc = None\n",
    "        for i in range(len(info_list)):\n",
    "            filename = info_list[i].filename.split(\".\")[0]\n",
    "            if report_idx == filename:\n",
    "                target_doc = info_list[i]\n",
    "                break\n",
    "        if target_doc == None:\n",
    "            print(target_doc, info_list)\n",
    "            assert target_doc != None, \"ERROR\"\n",
    "        return target_doc\n",
    "\n",
    "    return generate_parser(response, hook)\n",
    "\n",
    "\n",
    "# 기본 적인 다트 api 전송함수. 이 함수를 사용해서 다른 api 요청.\n",
    "def request_dart(url, params):\n",
    "    r = requests.get(url, params=params)\n",
    "    return r\n",
    "\n",
    "# api에 따라 파라미터 요청하는 경우가 있음. 거기에 맞춰서 요청. key와 value에 list 형태로 넣음.\n",
    "def create_params(keys = None, values= None):\n",
    "    params = {\n",
    "        'crtfc_key': api_key,\n",
    "    }\n",
    "\n",
    "    if keys:\n",
    "        assert len(keys) == len(values), \"key and value lengths must be same.\"\n",
    "        for i in range(len(keys)):\n",
    "            params[keys[i]] = values[i]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다트 데이터 받아옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다트의 전체 기업 리스트 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def request_dart_corps_info():\n",
    "    url = 'https://opendart.fss.or.kr/api/corpCode.xml'\n",
    "    params = create_params()\n",
    "    res = request_dart(url, params)\n",
    "    return res\n",
    "    \n",
    "def get_corp(only_stock = False):\n",
    "    res = request_dart_corps_info()\n",
    "\n",
    "    parser = create_get_corp_xml_parser(res)\n",
    "    corps_xml = parser.find_all(\"list\")\n",
    "\n",
    "    corps = []\n",
    "    num_stock = 0\n",
    "\n",
    "    for l in corps_xml:\n",
    "        corp_dict = {}\n",
    "        stock = None\n",
    "        if l.stock_code.string != \" \":\n",
    "            stock = l.stock_code.string\n",
    "            num_stock += 1\n",
    "\n",
    "        if ( only_stock == True and stock ) or only_stock == False:\n",
    "            corp_dict[\"주식 코드\"] = str(stock)\n",
    "            corp_dict[\"기업 코드\"] = str(l.corp_code.string)\n",
    "            corp_dict[\"기업 이름\"] = str(l.corp_name.string)\n",
    "            \n",
    "            corp_dict[\"수정 일자\"] = str(l.modify_date.string)\n",
    "            corps.append( corp_dict )\n",
    "    return corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업 정보\n",
    "# corps = get_corp(only_stock = True)\n",
    "# len(corps)\n",
    "# with open(\"all_corp_list.pickle\", \"wb\") as fp:   #Pickling\n",
    "  # pickle.dump(corps, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 받아온 기업 리스트에서 상장된 것만 추리기. 그리고 그 기업들에게서 최신 보고서 id을 받아온다.\n",
    "\n",
    "1. 그러기 위해서 각 기업의 정보를 받아옴\n",
    "2. 받아온 정보에서 stock code가 있으면 상장된 것\n",
    "3. 그 기업들에서 보고서 정보를 다시 받아옴\n",
    "4. 그 정보를 취합해서 다시 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업 보고서 id 받아오기 위해서 query 날릴 파라미터 설정\n",
    "def create_report_idx_params():\n",
    "    keys = ['bgn_de', 'pblntf_ty']\n",
    "    values = ['20210101', 'A']\n",
    "\n",
    "    params = create_params(keys, values)\n",
    "    return params\n",
    "\n",
    "def request_dart_corps_report_index(corp):\n",
    "    params = create_report_idx_params()\n",
    "    \n",
    "    code = corp['기업 코드']\n",
    "    params['corp_code'] = code\n",
    "    url = 'https://opendart.fss.or.kr/api/list.json'\n",
    "\n",
    "    res = request_dart(url, params)\n",
    "    return res\n",
    "\n",
    "def add_report_index_to_corp(latest_doc, corp):\n",
    "    act_corp = {}\n",
    "    for k, v in corp.items():\n",
    "        act_corp[k] = v\n",
    "        act_corp['report_idx'] = latest_doc['rcept_no']\n",
    "    return act_corp\n",
    "\n",
    "def get_repot_index_single(corp:str):\n",
    "    res = request_dart_corps_report_index(corp)\n",
    "    res = res.json()\n",
    "    if res['status'] == \"000\":\n",
    "        latest_doc = res['list'][0]\n",
    "        act_corp = add_report_index_to_corp(latest_doc, corp)\n",
    "        \n",
    "    else: # when the corp does not have report in range year 2021\n",
    "        # print(f\"{res['status']}: {res['message']}: {corp} \")\n",
    "        act_corp = corp\n",
    "    return act_corp\n",
    "\n",
    "def get_report_indexes(corps):\n",
    "    active_corps = []\n",
    "    non_active_corps = []\n",
    "    \n",
    "    for i, corp in tq.tqdm(enumerate(corps)):\n",
    "        try:\n",
    "            corp_result = get_repot_index_single(corp)\n",
    "        except Exception as e: # catch network error\n",
    "            print(e)\n",
    "            print(corp)\n",
    "\n",
    "        if corp_result:\n",
    "            active_corps.append( corp_result )\n",
    "        else:# when the corp does not have report in range year 2021\n",
    "            non_active_corps.append( corp_result )\n",
    "        LIMIT = 800\n",
    "        if i > 0 and i % LIMIT == 0:\n",
    "            print(f\"so far {len(active_corps)} has been collected...\")\n",
    "            print(f\"{LIMIT} requests has done. sleep 60 secs...\")\n",
    "\n",
    "            time.sleep(60)\n",
    "          \n",
    "    return active_corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # from copr information -> get latest report id\n",
    "# active_corps = get_report_index(corps)\n",
    "# len(active_corps)\n",
    "# with open(\"active_corps.pickle\", \"wb\") as fp:   #Pickling\n",
    "#   pickle.dump(active_corps, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상장된 기업에서 기업 보고서 요청해서 xml 받아오기.\n",
    "그리고 dict 처리해서 저장함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 요청하는 함수\n",
    "def request_dart_report(corp):\n",
    "    r = corp['report_idx']\n",
    "    keys = ['rcept_no']\n",
    "    values = [r]\n",
    "\n",
    "    params = create_params(keys, values)\n",
    "\n",
    "    url = 'https://opendart.fss.or.kr/api/document.xml'\n",
    "    r = requests.get(url, params=params)  \n",
    "    return r\n",
    "\n",
    "# 각 기업 마다 보고서 요청. \n",
    "def get_report(corp):\n",
    "    \"\"\"corp 1개에 대해서 공시 보고서 내용 긁어서 corp dict에 저장해서 반환\"\"\"\n",
    "      \n",
    "    try:\n",
    "        r = request_dart_report(corp)\n",
    "        soup = create_report_xml_parser(r, corp['report_idx'])\n",
    "        corp['original_xml'] = str(soup)\n",
    "        remove_table(soup)\n",
    "        # contents = parse_content_from_xml(soup)\n",
    "        # corp['contents'] = contents\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(corp)\n",
    "        corp['original_xml'] = None\n",
    "        # corp['contents'] = None\n",
    "    return corp\n",
    "    \n",
    "# 병렬화 하기 위해서 1개씩 들어오는지 확인\n",
    "def parallel_get_doc(active_corps):\n",
    "    assert isinstance(active_corps, dict)\n",
    "    active_corps = get_report(active_corps)\n",
    "    return active_corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from latest report id -> get xml and parse them. \n",
    "# without multiprocess, it takes about 1 hour...\n",
    "# pool = ProcessPool()\n",
    "# LIMIT = 900\n",
    "# content_orig_total = []\n",
    "# for i in tq.tqdm(range(0, len(active_corps), LIMIT)):\n",
    "#     print(f\"from {i} to {i+LIMIT}\")\n",
    "#     content_orig = pool.map(parallel_get_doc, active_corps[i:i+LIMIT])\n",
    "#     print(len(content_orig))\n",
    "#     content_orig_total.extend(content_orig)\n",
    "#     time.sleep(60)\n",
    "# with open(\"content_orig_total.pickle\", \"wb\") as fp:   #Pickling\n",
    "    # pickle.dump(content_orig_total, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e31c68abf1d5dd3f9e2269f23eadf1b199587e56c0618a30760176a65ebfcab4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
