{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration nlpHakdang___aihub-news30k-1ea472361ab2489f\n",
      "Reusing dataset csv (/opt/ml/.cache/huggingface/datasets/csv/nlpHakdang___aihub-news30k-1ea472361ab2489f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9594d467bb2438fb9e26ed278e6d164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "news_dataset = load_dataset('nlpHakdang/aihub-news30k',  data_files={\"train\":\"news_train_1_1.csv\", \"valid\":\"news_valid_1_1.csv\"}, use_auth_token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'category', 'media_sub_type', 'media_name', 'char_count', 'publish_date', 'title', 'original', 'article', 'extractive', 'abstract'],\n",
       "        num_rows: 271093\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'category', 'media_sub_type', 'media_name', 'char_count', 'publish_date', 'title', 'original', 'article', 'extractive', 'abstract'],\n",
       "        num_rows: 30122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting kobart\n",
      "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-uswpbj7g/kobart_80b8f982e04944aca699e9bfb94dd51d\n",
      "  Running command git clone --filter=blob:none -q https://github.com/SKT-AI/KoBART /tmp/pip-install-uswpbj7g/kobart_80b8f982e04944aca699e9bfb94dd51d\n",
      "  Resolved https://github.com/SKT-AI/KoBART to commit 1811d3779cf394872f32073d01c1201c8525bc00\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from kobart) (1.20.26)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from kobart) (1.1.5)\n",
      "Requirement already satisfied: pytorch-lightning==1.2.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from kobart) (1.2.1)\n",
      "Collecting torch==1.7.1\n",
      "  Using cached torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
      "Collecting transformers==4.3.3\n",
      "  Using cached transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.2.1->kobart) (4.62.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.2.1->kobart) (0.18.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.2.1->kobart) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.2.1->kobart) (1.21.4)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.2.1->kobart) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.2.1->kobart) (2021.11.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from torch==1.7.1->kobart) (3.10.0.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (0.0.46)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (2.26.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (4.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (0.10.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.3.3->kobart) (21.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from boto3->kobart) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.26 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from boto3->kobart) (1.23.26)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from boto3->kobart) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pandas->kobart) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pandas->kobart) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.26->boto3->kobart) (1.26.7)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (3.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->kobart) (1.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.19.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.0.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (58.0.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.3.3->kobart) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.3.3->kobart) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.3.3->kobart) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from importlib-metadata->transformers==4.3.3->kobart) (3.6.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from packaging->transformers==4.3.3->kobart) (2.4.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers==4.3.3->kobart) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers==4.3.3->kobart) (8.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (4.0.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (1.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.1.1)\n",
      "Building wheels for collected packages: kobart\n",
      "  Building wheel for kobart (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobart: filename=kobart-0.5.1-py3-none-any.whl size=9563 sha256=40420e875e72d48e7a837e153f97fba8646c8e3d33a4aac3d164d8dc3ceee74f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tri20swv/wheels/6e/55/c4/bd4fede223bc304089ac8da2a2099a69db3fcd4b0e853383f5\n",
      "Successfully built kobart\n",
      "Installing collected packages: torch, transformers, kobart\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.1\n",
      "    Uninstalling torch-1.10.1:\n",
      "      Successfully uninstalled torch-1.10.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.14.1\n",
      "    Uninstalling transformers-4.14.1:\n",
      "      Successfully uninstalled transformers-4.14.1\n",
      "  Attempting uninstall: kobart\n",
      "    Found existing installation: kobart 0.4\n",
      "    Uninstalling kobart-0.4:\n",
      "      Successfully uninstalled kobart-0.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers-domain-adaptation 0.3.1 requires datasets<1.3,>=1.2, but you have datasets 1.16.1 which is incompatible.\n",
      "transformers-domain-adaptation 0.3.1 requires tokenizers<0.10,>=0.9, but you have tokenizers 0.10.3 which is incompatible.\n",
      "torchvision 0.7.0 requires torch==1.6.0, but you have torch 1.7.1 which is incompatible.\n",
      "sentence-transformers 2.1.0 requires transformers<5.0.0,>=4.6.0, but you have transformers 4.3.3 which is incompatible.\n",
      "pororo 0.4.2 requires torch==1.6.0, but you have torch 1.7.1 which is incompatible.\n",
      "kobert 0.2.1 requires transformers>=4.8.1, but you have transformers 4.3.3 which is incompatible.\u001b[0m\n",
      "Successfully installed kobart-0.5.1 torch-1.7.1 transformers-4.3.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['â–ì•ˆë…•í•˜', 'ì„¸ìš”.', 'â–í•œêµ­ì–´', 'â–B', 'A', 'R', 'T', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ¤£', ':)', 'l^o']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobart import get_kobart_tokenizer\n",
    "kobart_tokenizer = get_kobart_tokenizer()\n",
    "kobart_tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ BART ì…ë‹ˆë‹¤.ğŸ¤£:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartModel\n",
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
    "\n",
    "kobart_tokenizer = get_kobart_tokenizer()\n",
    "model = BartModel.from_pretrained(get_pytorch_kobart_model())\n",
    "\n",
    "inputs = kobart_tokenizer(['ì•ˆë…•í•˜ì„¸ìš”.'], return_tensors='pt')\n",
    "outputs = model(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bart import BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "def load_model():\n",
    "    model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())\n",
    "    # tokenizer = get_kobart_tokenizer()\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "tokenizer = get_kobart_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16732/3045542214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFuncType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_DecoratorContextManager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Allow a context manager to be used as a decorator\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogitsProcessor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmodify\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mscores\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 \u001b[0mhead\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0mat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             \u001b[0mmax_length\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mpad_token_id\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0mbatch_beam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m         \u001b[0mbeam_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m         \u001b[0mbeam_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
     ]
    }
   ],
   "source": [
    "text = \"ì• í”Œì´ ë‚´ë…„ í•˜ë°˜ê¸° ì¶œì‹œí•  ì•„ì´í°14(ê°€ì¹­)ì— í€ì¹˜í™€(ì¹´ë©”ë¼ êµ¬ë©) ë””ìŠ¤í”Œë ˆì´ë¥¼ ì ìš©í•  ê²ƒì´ë¼ëŠ” ì „ë§ì´ ë‚˜ì˜¤ë©´ì„œ ì•„ì´í° ê³µê¸‰ë§ì— í¬í•¨ë¼ ìˆëŠ” ì‚¼ì„±ë””ìŠ¤í”Œë ˆì´, LGë””ìŠ¤í”Œë ˆì´, ì¤‘êµ­ BOEì˜ í‘œì •ì´ ì—‡ê°ˆë¦°ë‹¤. ì´ë¯¸ ê°¤ëŸ­ì‹œ ì‹œë¦¬ì¦ˆì— í€ì¹˜í™€ ìœ ê¸°ë°œê´‘ë‹¤ì´ì˜¤ë“œ(OLED) íŒ¨ë„ì„ ëŒ€ëŸ‰ ê³µê¸‰í•œ ì´ë ¥ì´ ìˆëŠ” ì‚¼ì„±ë””ìŠ¤í”Œë ˆì´ëŠ” ì—¬ìœ ë¡œìš´ ë¶„ìœ„ê¸°ê°€ ê°ì§€ëœë‹¤. LGë””ìŠ¤í”Œë ˆì´ëŠ” ê³µê¸‰ ê²½í—˜ì€ ì—†ì§€ë§Œ í€ì¹˜í™€ ë””ìŠ¤í”Œë ˆì´ì˜ ê¸°ìˆ  ë‚œì´ë„ê°€ ë†’ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ íŒë‹¨í•œë‹¤. ë°˜ë©´ ê³µê¸‰ ê²½í—˜ì´ ìˆì§€ë§Œ ë””ìŠ¤í”Œë ˆì´ ê¸°ìˆ ë ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” BOEëŠ” ì¼ë‹¨ ì• í”Œì´ ìš”êµ¬í•˜ëŠ” ê¸°ìˆ  ìˆ˜ì¤€ì„ ë”°ë¼ê°€ëŠ” ë° ì£¼ë ¥í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.17ì¼ í°ì•„ë ˆë‚˜ ë“± í•´ì™¸ ì •ë³´ê¸°ìˆ (IT) ì „ë¬¸ë§¤ì²´ ë“±ì— ë”°ë¥´ë©´ ì• í”Œì€ ë‚´ë…„ ì¶œì‹œí•  ì•„ì´í°14 ì‹œë¦¬ì¦ˆ ê³ ê¸‰ ëª¨ë¸ì¸ í”„ë¡œì˜ ì „ë©´ ë””ìì¸ì—ì„œ â€˜ë…¸ì¹˜â€™ë¥¼ ë¹¼ê³  í€ì¹˜í™€ì„ ì ìš©í•œë‹¤. ë…¸ì¹˜ëŠ” ë””ìŠ¤í”Œë ˆì´ ìƒë‹¨ì˜ ì¼ì • ë¶€ë¶„ì„ ì¹´ë©”ë¼ ëª¨ë“ˆê³¼ ì„¼ì„œ ë“±ì— í• ì• í•˜ëŠ” ë””ìì¸ìœ¼ë¡œ, ìƒê¹€ìƒˆê°€ í•œì â€˜ì˜¤ëª©í•  ìš”(å‡¹)â€™ìì™€ ìœ ì‚¬í•˜ë‹¤. ì´ ë•Œë¬¸ì— â€˜Mì íƒˆëª¨â€™ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. ì• í”Œì€ í˜ì´ìŠ¤IDë¼ëŠ” ì•ˆë©´ì¸ì‹ ë³´ì•ˆê¸°ìˆ ì„ ìœ„í•´ ë…¸ì¹˜ ë””ìì¸ì„ ì§€ë‚œ 2018ë…„ ì¶œì‹œí•œ ì•„ì´í°X(í…)ë¶€í„° ìµœê·¼ ë‚´ë†“ì€ ì•„ì´í°13ê¹Œì§€ ìœ ì§€í•˜ê³  ìˆë‹¤.\"\n",
    "#text = \"ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ê°€ ìµœê·¼ ì£¼ê°€ ê¸‰ë“±ì— í˜ì…ì–´ ë„¤ì´ë²„ë¥¼ ë°€ì–´ë‚´ê³  ì½”ìŠ¤í”¼ ì‹œê°€ì´ì•¡ 3ìœ„ì— ì˜¬ëë‹¤. 17ì¼ ì˜¤ì „ 9ì‹œ51ë¶„ ê¸°ì¤€ ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 96ë§Œ2,000ì›ì— ê±°ë˜ë˜ê³  ìˆë‹¤. ë§¥ì¿¼ë¦¬, CSì¦ê¶Œ ë“± ì™¸êµ­ê³„ ì°½êµ¬ë¥¼ í†µí•œ ìˆœë§¤ìˆ˜ì„¸ê°€ ìœ ì…ë˜ê³  ìˆë‹¤. ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ì „ì¼ì—ë„ 6% ê°€ê¹Œì´ ìƒìŠ¹ ë§ˆê°í–ˆìœ¼ë©° ì§€ë‚œ 14ì¼ë¶€í„° 4ê±°ë˜ì¼ ì—°ì† ìƒìŠ¹ì„¸ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ì‹œê°€ì´ì•¡ì€ í˜„ì¬ê°€ ê¸°ì¤€ 63ì¡°7,831ì–µ ì›ìœ¼ë¡œ ë„¤ì´ë²„ë¥¼ ì•ì§ˆë €ë‹¤. ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì™€ ë„¤ì´ë²„ì˜ ì‹œì´ ê²©ì°¨ëŠ” í˜„ì¬ 3,774ì–µ ì› ìˆ˜ì¤€ì´ë‹¤.\"\n",
    "if text:\n",
    "    text = text.replace('\\n', '')\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    output = model.generate(input_ids, eos_token_id=1, min_length=32, max_length=512, num_beams=5)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™” ì‹œìŠ¤í…œ í‰ê°€ , ë²ˆì—­ì˜ ì •ë‹µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /opt/ml/final-project-level3-nlp-06/summarize/.cache/kobert_v1.zip\n",
      "using cached model. /opt/ml/final-project-level3-nlp-06/summarize/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "model, vocab  = get_pytorch_kobert_model()\n",
    "sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n",
       "        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n",
       "        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-mkn6k4ek/kobert-tokenizer_e5ab57838c9c4671bf68d5cb457322d2\n",
      "  Running command git clone --filter=blob:none -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-mkn6k4ek/kobert-tokenizer_e5ab57838c9c4671bf68d5cb457322d2\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit d0c0829be97b3a363a8035d4131e8be4988e129d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b04e1321ca44389ebbaea2d3807aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3c949341b04e6f83d50473edbf6495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6cc47980d14516b3e9c5a5c0cd635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "tokenizer.encode(\"í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new_ones'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16732/371557594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mFuncType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFuncType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs_embeds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n\u001b[0;32m--> 905\u001b[0;31m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             )\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_attention_mask_for_generation\u001b[0;34m(self, input_ids, pad_token_id, eos_token_id, inputs_embeds)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     def _prepare_encoder_decoder_kwargs_for_generation(\n",
      "\u001b[0;32m/opt/conda/envs/lightweight/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "text = \"í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤.\"\n",
    "inputs = tokenizer.batch_encode_plus([text])\n",
    "output = model.generate(inputs, eos_token_id=1, max_length=64, num_beams=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, eos_token_id=1, max_length=64, num_beams=5)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ê°€ ìµœê·¼ ì£¼ê°€ ê¸‰ë“±ì— í˜ì…ì–´ ë„¤ì´ë²„ë¥¼ ë°€ì–´ë‚´ê³  ì½”ìŠ¤í”¼ ì‹œê°€ì´ì•¡ 3ìœ„ì— ì˜¬ëë‹¤. 17ì¼ ì˜¤ì „ 9ì‹œ51ë¶„ ê¸°ì¤€ ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” 96ë§Œ2,000ì›ì— ê±°ë˜ë˜ê³  ìˆë‹¤. ë§¥ì¿¼ë¦¬, CSì¦ê¶Œ ë“± ì™¸êµ­ê³„ ì°½êµ¬ë¥¼ í†µí•œ ìˆœë§¤ìˆ˜ì„¸ê°€ ìœ ì…ë˜ê³  ìˆë‹¤. ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ì „ì¼ì—ë„ 6% ê°€ê¹Œì´ ìƒìŠ¹ ë§ˆê°í–ˆìœ¼ë©° ì§€ë‚œ 14ì¼ë¶€í„° 4ê±°ë˜ì¼ ì—°ì† ìƒìŠ¹ì„¸ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ì‹œê°€ì´ì•¡ì€ í˜„ì¬ê°€ ê¸°ì¤€ 63ì¡°7,831ì–µ ì›ìœ¼ë¡œ ë„¤ì´ë²„ë¥¼ ì•ì§ˆë €ë‹¤. ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì™€ ë„¤ì´ë²„ì˜ ì‹œì´ ê²©ì°¨ëŠ” í˜„ì¬ 3,774ì–µ ì› ìˆ˜ì¤€ì´ë‹¤.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"skt/kobert-base-v1\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"skt/kobert-base-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the HuggingFace summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarized = summarizer(inputs[\"input_ids\"], min_length=75, max_length=300)\n",
    "\n",
    "# Print summarized text\n",
    "print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e31c68abf1d5dd3f9e2269f23eadf1b199587e56c0618a30760176a65ebfcab4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('lightweight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
