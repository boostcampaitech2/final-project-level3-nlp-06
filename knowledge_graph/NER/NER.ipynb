{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, AutoModelForTokenClassification\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_NER = pipeline('ner', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 1,\n",
      "  'score': 0.5727778,\n",
      "  'start': None,\n",
      "  'word': '▁정'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 2,\n",
      "  'score': 0.5071354,\n",
      "  'start': None,\n",
      "  'word': '동'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 3,\n",
      "  'score': 0.61839986,\n",
      "  'start': None,\n",
      "  'word': '환'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 4,\n",
      "  'score': 0.5251196,\n",
      "  'start': None,\n",
      "  'word': '▁선'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 5,\n",
      "  'score': 0.57170033,\n",
      "  'start': None,\n",
      "  'word': '착'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 6,\n",
      "  'score': 0.5087182,\n",
      "  'start': None,\n",
      "  'word': '을'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 7,\n",
      "  'score': 0.540753,\n",
      "  'start': None,\n",
      "  'word': '▁위해서'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 8,\n",
      "  'score': 0.5385701,\n",
      "  'start': None,\n",
      "  'word': '▁영'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 9,\n",
      "  'score': 0.61000824,\n",
      "  'start': None,\n",
      "  'word': '동'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 10,\n",
      "  'score': 0.58252746,\n",
      "  'start': None,\n",
      "  'word': '리'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 11,\n",
      "  'score': 0.5653955,\n",
      "  'start': None,\n",
      "  'word': '로'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 12,\n",
      "  'score': 0.5095013,\n",
      "  'start': None,\n",
      "  'word': '▁둔'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 13,\n",
      "  'score': 0.5865943,\n",
      "  'start': None,\n",
      "  'word': '갑'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 14,\n",
      "  'score': 0.5091945,\n",
      "  'start': None,\n",
      "  'word': '해'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 15,\n",
      "  'score': 0.5139626,\n",
      "  'start': None,\n",
      "  'word': '▁아름다운'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 16,\n",
      "  'score': 0.55067116,\n",
      "  'start': None,\n",
      "  'word': '▁'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 17,\n",
      "  'score': 0.52752584,\n",
      "  'start': None,\n",
      "  'word': 'X'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 18,\n",
      "  'score': 0.6105918,\n",
      "  'start': None,\n",
      "  'word': '마'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 19,\n",
      "  'score': 0.5040098,\n",
      "  'start': None,\n",
      "  'word': '스'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 20,\n",
      "  'score': 0.5554695,\n",
      "  'start': None,\n",
      "  'word': '▁'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 21,\n",
      "  'score': 0.56341624,\n",
      "  'start': None,\n",
      "  'word': '래'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 22,\n",
      "  'score': 0.55229944,\n",
      "  'start': None,\n",
      "  'word': '키'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 23,\n",
      "  'score': 0.5414368,\n",
      "  'start': None,\n",
      "  'word': '의'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 24,\n",
      "  'score': 0.5741462,\n",
      "  'start': None,\n",
      "  'word': '▁옛'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 25,\n",
      "  'score': 0.5674155,\n",
      "  'start': None,\n",
      "  'word': '생'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 26,\n",
      "  'score': 0.54932064,\n",
      "  'start': None,\n",
      "  'word': '각'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 27,\n",
      "  'score': 0.5364016,\n",
      "  'start': None,\n",
      "  'word': '을'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_1',\n",
      "  'index': 28,\n",
      "  'score': 0.50744635,\n",
      "  'start': None,\n",
      "  'word': '▁만들기'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 29,\n",
      "  'score': 0.5118707,\n",
      "  'start': None,\n",
      "  'word': '도'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 30,\n",
      "  'score': 0.5438742,\n",
      "  'start': None,\n",
      "  'word': '▁했'},\n",
      " {'end': None,\n",
      "  'entity': 'LABEL_0',\n",
      "  'index': 31,\n",
      "  'score': 0.6534741,\n",
      "  'start': None,\n",
      "  'word': '었습니다'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "lines=\"정동환 선착을 위해서 영동리로 둔갑해 아름다운 X마스 래키의 옛생각을 만들기도 했었습니다\"\n",
    "pp.pprint(nlp_NER(lines))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_input_file_to_tensor_dataset(lines,                                         \n",
    "                                         tokenizer,\n",
    "                                         pad_token_label_id,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "    # Setting based on the current model type\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    all_slot_label_mask = []\n",
    "    max_seq_len = 100\n",
    "\n",
    "    for words in lines:\n",
    "        tokens = []\n",
    "        slot_label_mask = []\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_label_mask.extend([0] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_len - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [pad_token_label_id]\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "        all_slot_label_mask.append(slot_label_mask)\n",
    "\n",
    "    # Change to Tensor\n",
    "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "    all_slot_label_mask = torch.tensor(all_slot_label_mask, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_slot_label_mask)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def get_labels(label_file):\n",
    "    return [label.strip() for label in open(os.path.join(label_file), 'r', encoding='utf-8')]\n",
    "def read_input_file(input_file):\n",
    "    lines = []\n",
    "    try : \n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                words = line.split()\n",
    "                lines.append(words)\n",
    "    except:\n",
    "        temp = \"부문  주요제품  CE 부문  TV, 모니터, 냉장고, 세탁기, 에어컨 등  IM 부문  HHP, 네트워크시스템, 컴퓨터 등  DS 부문  DRAM, NAND Flash, 모바일AP, 스마트폰용 OLED 패널 등   Harman 부문  디지털 콕핏(Digital Cockpit), 텔레매틱스(Telematics), 스피커 등\"\n",
    "        line = temp.strip()\n",
    "        words = temp.split()\n",
    "        lines.append(words)\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "def predict():\n",
    "    # load model and args    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForTokenClassification.from_pretrained('monologg/kobert', num_labels=30)  # Config will be automatically loaded from model_dir\n",
    "    model.to(device)\n",
    "    model.eval()    \n",
    "    label_lst = get_labels(\"./label.txt\")   \n",
    "    # Convert input file to TensorDataset\n",
    "    pad_token_label_id = torch.nn.CrossEntropyLoss().ignore_index    \n",
    "    lines = read_input_file(\"./pred_test.txt\")\n",
    "    dataset = convert_input_file_to_tensor_dataset(lines, tokenizer, pad_token_label_id)\n",
    "\n",
    "    # Predict\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    data_loader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n",
    "    all_slot_label_mask = None\n",
    "    preds = None\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"labels\": None}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                all_slot_label_mask = batch[3].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                all_slot_label_mask = np.append(all_slot_label_mask, batch[3].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    slot_label_map = {i: label for i, label in enumerate(label_lst)}\n",
    "    preds_list = [[] for _ in range(preds.shape[0])]\n",
    "\n",
    "    for i in range(preds.shape[0]):\n",
    "        for j in range(preds.shape[1]):\n",
    "            if all_slot_label_mask[i, j] != pad_token_label_id:\n",
    "                preds_list[i].append(slot_label_map[preds[i][j]])\n",
    "\n",
    "    # Write to output file\n",
    "    with open(\"ner_result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for words, preds in zip(lines, preds_list):\n",
    "            line = \"\"\n",
    "            for word, pred in zip(words, preds):\n",
    "                if pred == 'O':\n",
    "                    line = line + word + \" \"\n",
    "                else:\n",
    "                    line = line + \"[{}:{}] \".format(word, pred)\n",
    "\n",
    "            f.write(\"{}\\n\".format(line.strip()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
